### 输入 logits（原始分数）：
```text
[9.2, 3.5, -1.1, 7.8, 0.4]
```

### Step 1: 对每个分数取 exp（e^x）

text

```
exp(9.2)  ≈ 9897.129
exp(3.5)  ≈ 33.115
exp(-1.1) ≈ 0.333
exp(7.8)  ≈ 2440.602
exp(0.4)  ≈ 1.492
```

### Step 2: 求所有 exp 的和（分母）

text

```
总和 ≈ 9897.129 + 33.115 + 0.333 + 2440.602 + 1.492 = 12372.671
```

### Step 3: 每个位置的概率 = 自己的 exp / 总和

- 第1个（9.2）：9897.129 / 12372.671 ≈ **0.7999** → 约 **80%**
- 第2个（3.5）：33.115 / 12372.671 ≈ **0.0027** → 约 **0.3%**
- 第3个（-1.1）：0.333 / 12372.671 ≈ **0.00003** → 几乎 0%
- 第4个（7.8）：2440.602 / 12372.671 ≈ **0.1973** → 约 **19.7%**
- 第5个（0.4）：1.492 / 12372.671 ≈ **0.0001** → 几乎 0%

### 最终 Softmax 输出（四舍五入到小数点后两位）：

text

```
[0.80, 0.00, 0.00, 0.20, 0.00]
```

所以**精确值是 0.80（80%） + 0.20（20%）**，其他基本可以忽略。

**为什么 Softmax 用 exp(zᵢ) 做加权，而不是直接用 zᵢ 做加权平均？**

简单来说：因为**直接用原始分数 zᵢ 做加权平均，会出大问题**，而 exp 把问题完美解决了。下面一步步解释原因。

### 直接用 zᵢ 做加权平均会怎样？（假设不取 exp）

假如我们天真地写成：
```text
概率ᵢ = zᵢ / Σ zⱼ
```

会出现以下致命问题：

1. **负数分数会导致负概率** 很多 logits 是负的（比如 -5、-10），负数除以和 → 负概率！ 概率不能是负的，这直接违反概率的基本定义。
2. **分数可以是任意实数，尺度不一致** 一个模型可能输出 [100, 90, 80]，另一个输出 [2.5, 2.3, 2.1] 直接除和后，第一组概率几乎是 [0.33, 0.33, 0.33]（很平），第二组也差不多，但它们其实表达了完全不同的“置信度差异”。
3. **没有“winner-takes-all”效应** 直接平均时，分数高的优势不明显。 比如 [10, 9, 0] → 概率 ≈ [0.53, 0.47, 0] 看起来第一第二差不多，但实际上 10 和 9 的差距应该让第一个远超第二个。

### exp 解决了所有这些问题，而且带来了很好的性质

用 exp(zᵢ) 后，情况完全不同：

|问题点|直接用 zᵢ 加权平均|用 exp(zᵢ) 加权平均（Softmax）|为什么 exp 更好？|
|---|---|---|---|
|负数|会产生负概率|exp(负数) 永远 > 0|概率永远非负|
|尺度敏感性|分数整体大或小影响很大|exp 让大分数指数级放大，小分数指数级压缩|相同相对差距在大分数时被极度放大（winner-takes-most）|
|置信度差异的表达|线性关系，差距不明显|指数关系，差距被指数级拉大|模型越确定（分数越高），概率越接近 1，其他越接近 0|
|数值稳定性|容易受极大/极小值影响|exp 后大值主导，小值几乎忽略|自然实现了“关注最可能的，忽略不可能的”|
|数学性质|不保证和为 1（如果有负数更糟）|严格保证 Σ softmax(zᵢ) = 1|满足概率分布的基本公理|

### 最直观的对比（指数级的放大效应）

假设两个选项，分数差 5：

|分数对|直接平均概率|Softmax 概率|实际含义|
|---|---|---|---|
|[5, 0]|[0.83, 0.17]|[0.993, 0.007]|第一项几乎确定赢|
|[10, 5]|[0.83, 0.17]|[0.993, 0.007]|差距更大，但 Softmax 还是差不多 99.3%|
|[1, -4]|[负概率！非法]|[0.993, 0.007]|负分几乎被抹平，仍然合理|

你会发现：**Softmax 的概率对分数差距非常敏感**，分数稍微高一点，概率就指数级飙升，这正是我们希望的——模型越“确定”，输出概率越尖锐。

### 总结：为什么一定要 exp？

exp 不是随意选的，它几乎是唯一能同时满足下面所有需求的函数：

1. 把任意实数映射到 (0, +∞)
2. 单调递增（分数越高概率越高）
3. 指数级放大差距（winner-takes-most）
4. 让求和后归一化变成合法概率

**所以 Softmax 用 exp 的加权平均，本质上是“用指数函数来实现置信度的非线性放大 + 保证概率合法性”的最优雅、最有效的办法。**

几乎所有现代深度学习模型（分类、语言模型、注意力、蒸馏、RL 等）都依赖这个性质，所以 exp 几乎成了“铁打的标准”。没有更好的替代品（至少目前没有）。
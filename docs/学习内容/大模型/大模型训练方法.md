大语言模型（LLM）的训练和优化方法整体框架已经相对成熟，但2026年初的实际趋势比2023–2024年的“经典教科书”pipeline更灵活、更高效。经典的三阶段（**预训练 → SFT → RLHF with PPO**）仍然存在，但现在**DPO家族主导开源社区**，**PPO更多用于顶级闭源或极致精细对齐**，而**轻量无参数方法（如提示工程 + RAG）**已成为大多数实际项目的首选起点。

我把所有常见方法按**“改动程度 & 资源消耗”从低到高**重新整理成一个清晰的光谱（2026年视角），方便你一目了然。每个类别包括核心思路、代表方法、优缺点、适用场景，以及当前主流地位。

### LLM 适配/优化方法光谱（2026年实用排序）

|层级|是否改模型参数|资源消耗|代表方法|核心思路|优点|缺点|当前主流地位 & 适用场景|
|---|---|---|---|---|---|---|---|
|1. 纯推理时优化（最轻量）|否|极低（几乎0额外训练）|提示工程（Prompt Engineering） Chain-of-Thought (CoT) ReAct / Tree-of-Thoughts Self-Consistency|只改输入提示、few-shot示例、系统指令，不碰权重|零成本、秒级迭代、可热更新、立即见效|效果上限受模型本身限制、长提示易超上下文|**首选起点**。几乎所有项目先试这个。适合快速原型、简单任务、推理增强。|
|2. 外部增强/架构级（轻量“拟微调”）|否|低（需建索引/工具）|RAG（Retrieval-Augmented Generation） 知识图谱增强 Tool Use / Function Calling / Agents|给模型配“外脑”（检索文档、调用工具），推理时注入上下文|知识实时更新、防幻觉强、可替换部分微调、知识可热插拔|检索质量依赖、延迟稍高、多跳推理仍弱|**非常主流**。2026年RAG仍是知识密集任务标配，常与提示工程组合。适合企业内部知识库、最新资讯、减少幻觉场景。经典RAG正逐步被更智能的agent式RAG取代。|
|3. 参数高效微调（PEFT，中量）|是（<1%参数）|中低（单/双卡可训70B+）|LoRA / QLoRA Adapter Prefix-Tuning / Prompt-Tuning|只训少量附加参数，原权重冻结|省显存、训练快、易合并/切换任务、效果接近全量|极致性能略逊全参数、需选好基模型|**开源社区标配**。几乎所有SFT/DPO都用QLoRA。适合资源有限的团队、领域适配、多任务切换。|
|4. 监督/指令微调（SFT，中量）|是（全/中量）|中|指令微调（Instruction Tuning） 领域微调|用“指令-回答”对继续预训练，让模型懂任务格式|快速提升指令跟随、专业性、格式一致|易过拟合、数据质量敏感|**必经阶段**。所有对齐前几乎都先SFT。2026年强调高质量小数据集（5k–50k条）+合成数据。|
|5. 偏好对齐/强化学习（Alignment，重）|是|高|PPO-based RLHF（经典） DPO / ORPO / SimPO / KTO（主流） GRPO / RTO（新兴变体） RLVR（可验证奖励）|用偏好数据（好>坏）或客观奖励优化策略|显著提升帮助性、安全性、价值观对齐|PPO复杂、不稳定、贵；DPO简单但上限稍低|**分化严重**： - 开源/中等项目：**DPO家族绝对主流**（稳定、快、无需奖励模型） - 顶级闭源/安全极致：仍用**PPO/GRPO** + 过程监督 + 在线RL RLVR在数学/代码/工具领域爆发。|
|6. 高级/混合/迭代（最高端）|是 + 系统|极高|在线RLHF/RLAIF 过程监督（Process Reward） 混合：SFT → DPO → PPO精调 MoE + 蒸馏 + 推理时扩展|持续迭代、上线后反馈循环、多阶段混合|效果上限最高、可动态更新|成本巨大、工程复杂|**商业顶级模型**（如o1式推理模型、Claude等）常用。2026年重点向**推理时扩展**（inference-time scaling）转移，训练侧进步占比下降。|

### 2026年最务实的典型pipeline推荐（按资源分）

- **资源极少（个人/单卡）** 提示工程 + RAG → 如果不够 → QLoRA + SFT → DPO/ORPO（用合成偏好数据）
- **中等资源（4–16卡，想开源级SOTA）** 预训练Base → SFT（高质量指令，PEFT） → DPO/SimPO（主） → 可选少量GRPO/PPO做安全/精细对齐
- **顶级资源/商业级** SFT → 奖励模型（人类+AI混合） → PPO/GRPO + 过程奖励 + 在线迭代 + 大量红队测试

### 一句话总结当前趋势（2026年1月）

“**能用提示+RAG就不微调，能DPO就不PPO，能过程/可验证奖励就不只结果偏好**。”

经典的“预训练 → SFT → PPO RLHF”仍然是教科书，但实际工程里已演变为更务实、更模块化的组合。轻量“拟微调”（提示工程 + RAG）确实被合理归为一类，因为它们是大多数场景下“**不改权重却能大幅提升**”的首选方案。